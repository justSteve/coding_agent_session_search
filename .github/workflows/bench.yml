# .github/workflows/bench.yml
# Performance benchmarks using Criterion with regression detection
#
# Features (T5.3):
# - Metric-specific thresholds: latency (10%), duration (20%), memory (15%), throughput (10%)
# - Historical trend tracking across runs
# - Test suite duration and memory tracking
name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          fetch-depth: 0  # For comparing with main

      - name: Install Rust nightly
        uses: dtolnay/rust-toolchain@881ba7bf39a41cda34ac9e123fb41b44ed08232f # nightly

      - name: Cache cargo
        uses: Swatinem/rust-cache@ad397744b0d591a723ab90405b7247fac0e6b8db # v2

      - name: Setup Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5
        with:
          python-version: '3.11'

      - name: Cache benchmark baselines
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: target/criterion
          key: criterion-${{ runner.os }}-${{ hashFiles('benches/**') }}
          restore-keys: |
            criterion-${{ runner.os }}-

      - name: Cache benchmark history
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: target/perf_history.json
          key: perf-history-${{ runner.os }}
          restore-keys: |
            perf-history-${{ runner.os }}

      - name: Run benchmarks (save baseline for main)
        if: github.ref == 'refs/heads/main'
        id: bench-main
        run: |
          start_time=$(date +%s%3N)
          cargo bench --bench index_perf --bench runtime_perf --bench search_perf -- --save-baseline main
          end_time=$(date +%s%3N)
          echo "bench_duration_ms=$((end_time - start_time))" >> $GITHUB_OUTPUT

      - name: Run benchmarks (compare with baseline for PRs)
        if: github.event_name == 'pull_request'
        id: bench-pr
        run: |
          start_time=$(date +%s%3N)
          cargo bench --bench index_perf --bench runtime_perf --bench search_perf -- --save-baseline pr
          end_time=$(date +%s%3N)
          echo "bench_duration_ms=$((end_time - start_time))" >> $GITHUB_OUTPUT

      - name: Save benchmark history (main branch)
        if: github.ref == 'refs/heads/main'
        run: |
          python scripts/check_bench_regression.py \
            --save-history \
            --history-file target/perf_history.json \
            --run-id "${{ github.sha }}" \
            --baseline main \
            --current main

      - name: Check for regressions (metric-specific thresholds)
        if: github.event_name == 'pull_request'
        run: |
          # Uses metric-specific thresholds:
          # - Latency (search): 10%
          # - Duration (test suite): 20%
          # - Memory: 15%
          # - Throughput (indexing): 10%
          python scripts/check_bench_regression.py \
            --latency-threshold 10 \
            --duration-threshold 20 \
            --memory-threshold 15 \
            --throughput-threshold 10 \
            --json > target/regression_report.json || true

          # Pretty print results
          python scripts/check_bench_regression.py \
            --latency-threshold 10 \
            --duration-threshold 20 \
            --memory-threshold 15 \
            --throughput-threshold 10

      - name: Analyze historical trends
        if: always()
        run: |
          if [ -f target/perf_history.json ]; then
            echo "## Trend Analysis" >> $GITHUB_STEP_SUMMARY
            python scripts/check_bench_regression.py \
              --analyze-trends \
              --history-file target/perf_history.json \
              --trend-window 5 \
              --json > target/trend_analysis.json 2>/dev/null || true

            if [ -f target/trend_analysis.json ]; then
              echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
              cat target/trend_analysis.json | head -50 >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show metric-specific thresholds
          echo "### Regression Thresholds (T5.3)" >> $GITHUB_STEP_SUMMARY
          echo "| Metric Type | Threshold |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------|-----------|" >> $GITHUB_STEP_SUMMARY
          echo "| Latency (search) | 10% |" >> $GITHUB_STEP_SUMMARY
          echo "| Duration (test suite) | 20% |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory | 15% |" >> $GITHUB_STEP_SUMMARY
          echo "| Throughput (indexing) | 10% |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show benchmark duration
          if [ -n "${{ steps.bench-main.outputs.bench_duration_ms }}" ]; then
            echo "### Timing" >> $GITHUB_STEP_SUMMARY
            echo "- Benchmark suite duration: ${{ steps.bench-main.outputs.bench_duration_ms }}ms" >> $GITHUB_STEP_SUMMARY
          elif [ -n "${{ steps.bench-pr.outputs.bench_duration_ms }}" ]; then
            echo "### Timing" >> $GITHUB_STEP_SUMMARY
            echo "- Benchmark suite duration: ${{ steps.bench-pr.outputs.bench_duration_ms }}ms" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show regression report if present
          if [ -f target/regression_report.json ]; then
            echo "### Regression Check" >> $GITHUB_STEP_SUMMARY
            has_regressions=$(jq -r '.has_regressions' target/regression_report.json)
            if [ "$has_regressions" = "true" ]; then
              echo "âš ï¸ **Regressions detected:**" >> $GITHUB_STEP_SUMMARY
              jq -r '.regressions[] | "- \(.name) [\(.metric_type)]: +\(.diff_pct | . * 10 | round / 10)% (threshold: \(.threshold)%)"' target/regression_report.json >> $GITHUB_STEP_SUMMARY
            else
              echo "âœ… No significant regressions detected" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show improvements
            improvements=$(jq -r '.improvements | length' target/regression_report.json)
            if [ "$improvements" -gt 0 ]; then
              echo "### Improvements" >> $GITHUB_STEP_SUMMARY
              jq -r '.improvements[] | "- \(.name) [\(.metric_type)]: \(.diff_pct | . * 10 | round / 10)%"' target/regression_report.json >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "ðŸ“Š Detailed reports available in workflow artifacts." >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark reports
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        if: always()
        with:
          name: benchmark-reports
          path: |
            target/criterion/
            target/perf_history.json
            target/regression_report.json
            target/trend_analysis.json
          retention-days: 30
